{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CE706 - Information Retrieval\n",
    "### Assignment 1: Indexing for Web Search\n",
    "#### Ajith Vajrala(1802560), Cynthia Masetto(1802774)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b76fa351b7a785825e8348aecf46998a69535f43"
   },
   "source": [
    "links: http://www.ardendertat.com/2011/07/17/how-to-implement-a-search-engine-part-3-ranking-tf-idf/\n",
    "\n",
    "https://stevenloria.com/tf-idf/\n",
    "\n",
    "\n",
    "http://aimotion.blogspot.com/2011/12/machine-learning-with-python-meeting-tf.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c1e7b8ff6afa76d686bef45c4f4149cd40d85e2"
   },
   "source": [
    "**The Task**\n",
    "\n",
    "* Your task is to apply your IR skills to build a processing pipeline that turns a Web site into structured knowledge\n",
    "(thus enhancing your chances of getting the job outlined above). Your system should take HTML pages as input, hts://careers.microsoft.com/us/en/job/580012/Software-Engineer process them using the kind of techniques that we have been looking at in the module, and output an index of\n",
    "terms identified in the documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab10c65fe613f583996ac7617876958363fd8430"
   },
   "source": [
    "• Engineering a Complete System (10%) The system you develop must be able to read Web pages\n",
    "from a specified set of URLs and produce appropriately formatted output. The Web pages should be\n",
    "processed one at a time using the steps outlined below. The final system should have control over all the\n",
    "individual components so that there is a single call and all the steps outlined below will be performed.\n",
    "\n",
    "• HTML Parsing (10%) Before the text can be analyzed it is necessary to get rid of the HTML tags.\n",
    "The result will be plain text. Note that if you simply delete all HTML tags, you will lose information\n",
    "such as meta tag keywords. Use an appropriate tool to perform this task.\n",
    "\n",
    "• Pre-processing: Sentence Splitting, Tokenization and Normalization (10%) The next step\n",
    "should be to transform the input text into a normal form of your choice.\n",
    "\n",
    "• Part-of-Speech Tagging (10%) The input should be tagged with a suitable part-of-speech tagger, so\n",
    "that the result can then be processed in the next steps.\n",
    "\n",
    "• Selecting Keywords and Phrases (20%) One aim of your system is to identify the words and phrases\n",
    "in the text that are most useful for indexing purposes. Your system should remove words which are not\n",
    "useful, such as very frequent words or stopwords, and identify phrases suitable as index terms. Apply\n",
    "tf.idf as part of your selection and weighting step.\n",
    "\n",
    "• Entity and Relation Extraction (20%) Apply a named-entity-recogniser (NER tagger) to your text\n",
    "to identify entities. Extract at least person names, locations and organisations. For additional marks also\n",
    "identify relations that hold between these entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "27fe32baf07f35d2da024650d4ed0ade61fd3ea4"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import conlltags2tree, tree2conlltags\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tree import Tree\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import httplib2\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "93d78093e28de6124308f5ca136b10310203ad5c"
   },
   "outputs": [],
   "source": [
    "#______________links in Page, code changes starts____________________# \n",
    "def get_links(url):\n",
    "    #read the url\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(resp, from_encoding=resp.info().get_param('charset'))\n",
    "    links =[]\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        links.append(link['href'])\n",
    "    #return all links present in that url\n",
    "    return links\n",
    "#______________links in Page, code changes End____________________# \n",
    "\n",
    "\n",
    "#_____________Get Meta Data, code changes start______________________#\n",
    "def get_metadata(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    metas = soup.find_all('meta')\n",
    "    meta_data =  [ meta.attrs['content'] for meta in metas if 'name' in meta.attrs and meta.attrs['name'] == 'description' ]\n",
    "    meta_data = [w.rstrip() for w in meta_data]\n",
    "    meta_data = [w.replace('\\n',' ') for w in meta_data] \n",
    "    return meta_data\n",
    "#_______________Get Meta Data, code changes end_____________________#\n",
    "\n",
    "\n",
    "\n",
    "#_____________HTML Parsing, code changes starts____________________# \n",
    "def url_to_string(url):\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    for script in soup([\"script\", \"style\", 'aside']):\n",
    "        script.extract()\n",
    "    data = \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
    "    return data\n",
    "#_____________HTML Parsing, code changes END____________________# \n",
    "\n",
    "\n",
    "#_____________Extracting nouns, noun forms in document, code changes start__________#\n",
    "def extract_noun_forms(data): \n",
    "    nouns_list = []\n",
    "    sentences = sent_tokenize(data)\n",
    "    for sent in sentences:\n",
    "        #' '.join((e for e in sent if e.isalnum())\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(tokens)    #Part-of-Speech Tagging'\n",
    "        nouns = [word for word, pos in tagged if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "        nouns_list.append(nouns)\n",
    "    nouns_list = [x for sublist in nouns_list for x in sublist]\n",
    "    return nouns_list\n",
    "#_____________Extracting nouns in document, code changes END__________#\n",
    "\n",
    "\n",
    "#_____________Entity and Relation Extraction, code changes starts____________________#\n",
    "# i-o-b tagging, places, organisations, Names, Landmarks tagging\n",
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    return continuous_chunk\n",
    "#_____________Entity and Relation Extraction, code changes End____________________#\n",
    "\n",
    "\n",
    "#_____________NAMED Entity Recognition and Extraction using spacy, code changes End____________________#\n",
    "def generate_ner_tags(doc):\n",
    "    out_list =[]\n",
    "    for word in doc:\n",
    "        #entities[word] = str(i.ent_iob_) + \"-\"+ str(i.ent_type_)\n",
    "        if word.ent_type_ in [\"ORG\", \"EVENT\", \"PERSON\", \"NORP\", \"GPE\", \"MONEY\", \"LOC\", \"WORK_OF_ART\"]:\n",
    "            my_list = [word, str(word.ent_iob_) + \"-\" + str(word.ent_type_)]\n",
    "            out_list.append(my_list)            \n",
    "    return out_list\n",
    "#_____________NAMED Entity Recognition and Extraction using spacy, code changes End____________________#\n",
    "\n",
    "\n",
    "\n",
    "#________________Text pre-processing code changes starts_______________#\n",
    "stop_words=set(stopwords.words(\"english\"))  \n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "pattern = r'[^a-zA-z0-9\\s]' #include digits  #r'[^a-zA-z\\s]' remove digits also\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "#Pre-processing: Sentence Splitting, Tokenization and Normalization \n",
    "def pre_process(text):\n",
    "    text = re.sub(pattern, '', text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_tokens = [lancaster.stem(word) for word in filtered_tokens]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "#_______________Text pre-processing code changes end_____________#\n",
    "\n",
    "\n",
    "#______________Saving Files, code changes start_____________#\n",
    "def save_text_file(filename, data):\n",
    "    with open(filename, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "            \n",
    "def save_json(filename, data):\n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(data, fp)\n",
    "        \n",
    "def save_html(file_name,html):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "         f.write(html)\n",
    "#_____________Saving File, code changes End_____________#\n",
    "\n",
    "\n",
    "\n",
    "#____________TF-IDF Updated Code changes start___________________#\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn):\n",
    "    #get the feature names and tf-idf score of top n items\n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    return results       \n",
    "\n",
    "def get_tf_idf(doc, corpus, num):\n",
    "    cv=CountVectorizer(stop_words=stop_words)\n",
    "    word_count_vector=cv.fit_transform(corpus)\n",
    "    \n",
    "    #TfidfTransformer to Compute Inverse Document Frequency (IDF)\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "    \n",
    "    feature_names= cv.get_feature_names()\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "     #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "     #extract only the top n;\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,num)\n",
    "    return keywords\n",
    "#____________________TF IDF updated Code changes end_________________#\n",
    "\n",
    "\n",
    "\n",
    "#____________Web search using cosine similarity, Code changes start_______________#\n",
    "def web_search(text):\n",
    "    cv=CountVectorizer( stop_words=stop_words)\n",
    "    word_count_vector=cv.fit_transform(corpus)\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "    full_x = tfidf_transformer.transform(cv.transform(corpus))\n",
    "    sample1 = tfidf_transformer.transform(cv.transform([pre_process(text)]))\n",
    "    cosine_similarities = linear_kernel(sample1,full_x).flatten()\n",
    "    #return the first link that closely matches the text entered and the corresponding links metadata \n",
    "    url = urls[cosine_similarities.argsort()[:-5:-1][0]] \n",
    "    return  url, get_metadata(url)\n",
    "#____________Web search using cosine similarity, Code changes End_______________#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "bcf4a12c26b72f4b5c8cfc36639ef493fb9e6522"
   },
   "outputs": [],
   "source": [
    "url1 = \"https://csee.essex.ac.uk/staff/udo/index.html\"\n",
    "url2 = \"https://www.essex.ac.uk/departments/computer-science-and-electronic-engineering\"\n",
    "\n",
    "urls = [url1, url2]\n",
    "\n",
    "data = []\n",
    "#HTML Parsing and saving all the text in data\n",
    "for i, url in enumerate(urls):\n",
    "    filename =\"html_parsed_doc_\" + str(i+1)\n",
    "    text = url_to_string(url)\n",
    "    save_text_file(filename, sent_tokenize(text))\n",
    "    data.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "52436c688d10550a023230bbe49a0828fc0e0d00"
   },
   "outputs": [],
   "source": [
    "#saving meta data present in each link\n",
    "for i, dat in enumerate(urls):\n",
    "    filename =\"Meta_data_in_link_\" + str(i+1)\n",
    "    text = get_metadata(url)\n",
    "    save_text_file(filename, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "cd62fea619eb723737b49af6db651e56902e51f9"
   },
   "outputs": [],
   "source": [
    "#saving all the links present in each document\n",
    "for i,url in enumerate(urls):\n",
    "    filename =\"links_in_doc_\" + str(i+1)\n",
    "    text = get_links(url)\n",
    "    save_text_file(filename, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "472d8e6ce22b704a1cb5a692f1d43f084f25bbf2"
   },
   "outputs": [],
   "source": [
    "#saving all the Named Entities present in each document\n",
    "for i,dat in enumerate(data):\n",
    "    filename =\"Named_Entities_in_doc_\" + str(i+1)\n",
    "    text = get_continuous_chunks(dat)\n",
    "    save_text_file(filename, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b0a9f0a724a4850160855273fedf196ca0fc27ef"
   },
   "outputs": [],
   "source": [
    "#NER recognition using spacy and saving importnat tags like PERSON, LOR, ORG, \n",
    "for i, dat in enumerate(data):\n",
    "    doc = nlp(data[i])\n",
    "    entities = generate_ner_tags(doc)\n",
    "    filename = \"NER_in_doc_\" + str(i+1)\n",
    "    save_text_file(filename, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "0db092cd52c43c9a1325f33379ead9673242cf42"
   },
   "outputs": [],
   "source": [
    "#saving all the Nouns and noun forms present in each document\n",
    "for i, dat in enumerate(data):\n",
    "    filename =\"Noun_and_noun_forms_in_doc_\" + str(i+1)\n",
    "    text = extract_noun_forms(dat)\n",
    "    save_text_file(filename, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "2674d1373a287816e5452ce3aeac5cfd40fb804e"
   },
   "outputs": [],
   "source": [
    "#Extracting corpus for calculating tf-idf scores\n",
    "corpus =[]\n",
    "for dat in data:\n",
    "    corpus.append(pre_process(dat))\n",
    "    \n",
    "#saving all the TF-IDF scores in each document\n",
    "for i, dat in enumerate(corpus):\n",
    "    keywords = get_tf_idf(dat, corpus, num=20)\n",
    "    filename = \"top_tf_idf_scores_in_doc_\" + str(i+1) + \".json\"\n",
    "    save_json(filename, keywords)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "62d365400e5e461ac28cdb328934bdc7ca98f018"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://csee.essex.ac.uk/staff/udo/index.html',\n",
       " ['Udo Kruschwitz, Web search technology, search engines, YPA, Information Retrieval, IR, Knowledge Extraction, Ontologies, Research Information, Intelligent Document Retrieval, Springer book, Natural Language Engineering, The Udo'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"Udo\"\n",
    "web_search(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "79d4ccd5ffcf782c63ee436323008512a336b40a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://www.essex.ac.uk/departments/computer-science-and-electronic-engineering',\n",
       " ['We offer a variety of degrees in computer science and electronic engineering, from computer games and robotics, to communications engineering'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"robotics\"\n",
    "web_search(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "ab4648a04262f1cbf736af3096be9b7f4b804c88"
   },
   "outputs": [],
   "source": [
    "#saving NER visually as html files to view whole data in page and corresponding tags\n",
    "for i, dat in enumerate(data):\n",
    "    doc = nlp(dat)\n",
    "    html = displacy.render(doc, style='ent', page=True)\n",
    "    filename = \"NER_Image_in_doc_\" + str(i+1) + \".html\"\n",
    "    save_html(filename, html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "41b24999d5c29392452be53cf40f30bd05fb73d7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
